import Complexity.Features.LexicalFeatures
import Complexity.TextDocument._
import Complexity.TextDocument
import edu.arizona.sista.processors.corenlp.CoreNLPProcessor
import Complexity.Utils.Testing._
import edu.arizona.sista.struct.Counter

import scala.collection.parallel.immutable.ParVector

/**
  * Created by mcapizzi on 3/25/16.
  */
class DocumentTests extends GeneralTest {

  //file to use for test
  val file = "0001AL_OwlAndMoon.txt"
  val annotation = "0001AL_OwlAndMoon.annotated"

  //create processor
  val p = new CoreNLPProcessor(withDiscourse = true)

  //make ProcessedParagraphs
  val procParsFromText = makeProcParsFromText(file)

  //annotate
  procParsFromText.foreach(_.annotate)

  //import annotation
  val procParsFromAnnotation = makeProcParsFromAnnotation(file, annotation)

  //make TextDocument
  val td = new TextDocument(procParsFromText)

  //features
  val lex = new LexicalFeatures(td)

  ///////tests///////

  "Imported annotation and annotated imported text" should "be the same" in {
    assert(procParsFromAnnotation == procParsFromText)
  }

  "Words without punctuation" should "be shorter than words with punctuation" in {
    for (paragraph <- procParsFromText) {
      val withPunct = paragraph.words(withPunctuation = true).flatten
      val withoutPunct = paragraph.words(withPunctuation = false).flatten
      assert(withPunct.length > withoutPunct.length)
    }
  }

  "The proper nouns" should "be Owl and Moon" in {
    assert(td.properNounsCounter.size == 2)
  }

  "The tags counter" should "have the same value whether generated by .foldApply or by .foldApplyCounter" in {
    val tagCounterList = td.parParagraphs.map(_.buildCounters).map(_._1("tags"))
    val generalFold = foldApply(tagCounterList, new Counter[String], (a: Counter[String],b: Counter[String]) => a + b)
    val counterFold = foldApplyCounter(tagCounterList, new Counter[String], (a,b) => a + b)


  }

  "Filtered stop-word counter " should "not contain determiners or conjunctions" in {
    assert(
      !td.filterStopWords("words").keySet.map(_._2).contains("DT") &&
      !td.filterStopWords("lemmas").keySet.map(_._2).contains("DT") &&
      !td.filterStopWords("words").keySet.map(_._2).contains("CC") &&
      !td.filterStopWords("lemmas").keySet.map(_._2).contains("CC")
    )
  }

  "Lexical feature vector" should "have normalized values less than one" in {
    val features = lex.makeLexicalFeatureVector

    assert(features.slice(2,9).filter(z => z._2 > 1) == Vector())
  }




}

